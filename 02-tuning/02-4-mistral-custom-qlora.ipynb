{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64c0e4cd",
   "metadata": {},
   "source": [
    "# 02-4: QLoRA tuning of Mistral-7B with custom dataset\n",
    "\n",
    "This Colab needs GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a26ee6-16f3-4be4-b131-75ec5f8713c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5b3037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "print(f\"xformers {version('xformers')}, datasets {version('datasets')}, bitsandbytes {version('bitsandbytes')}, accelerate {version('accelerate')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652f18b-0c64-433f-95d3-1151e3304c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wandb configuration\n",
    "import wandb\n",
    "wandb.login()  \n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"qlora-tests\"\n",
    ")\n",
    "wandb.run.name = \"qlora-mistral\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324768ae",
   "metadata": {},
   "source": [
    "## Load quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20570fef-8ac3-4cfe-8230-7b1f6c69adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", \n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557f76cc",
   "metadata": {},
   "source": [
    "## Add LoRa layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f83fc7b",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed885d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files='./vertexai-qna500.jsonl', split=\"train\")#, field='data')\n",
    "\n",
    "def generate_prompt_mistral(examples):\n",
    "    text = [\n",
    "        {\"role\": \"user\", \"content\": examples[\"input_text\"][10:]}, # remove \"question:\"\n",
    "        {\"role\": \"assistant\", \"content\": examples[\"output_text\"]}\n",
    "    ]\n",
    "\n",
    "    return text\n",
    "text_column =[generate_prompt_mistral(data_point) for data_point in dataset]\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    return {\"text\": tokenizer.apply_chat_template(examples[\"text\"], tokenize=False)}\n",
    "\n",
    "# Add the text_column to the dataset before mapping\n",
    "dataset = dataset.add_column(\"text\", text_column)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "print(dataset[5][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cb4390",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b1a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Set SFT `TrainingArguments`\n",
    "\n",
    "\n",
    "# TODO: Set `SFTTrainer` parameters\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Stop sending metrics to wandb\n",
    "wandb.finish()\n",
    "\n",
    "\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "\n",
    "# TODO: Run inference with first model (without QLoRA)\n",
    "\n",
    "# TODO: Activate QLoRA adapter and run inference again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee414c",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dac02c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"human\", \"content\": \"what is vertex AI\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8852d5c1",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb6844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save model"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m110"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
