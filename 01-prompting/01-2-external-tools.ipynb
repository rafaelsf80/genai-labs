{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ5caKL2Ff2B"
      },
      "source": [
        "# 01-2: External tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mhA8gbnohLo7"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = YOUR_PROJECT_ID  # <--- CHANGE THIS\n",
        "LOCATION = \"us-central1\"  # <--- CHANGE THIS\n",
        "MODEL_NAME = \"text-bison@002\" # <--- OPTIONALLY CHANGE HIS. OUTPUTS MAY DIFFER SIGNIFICANTLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "vertexai.init(project=PROJECT_ID,\n",
        "              location=LOCATION)\n",
        "parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_output_tokens\": 1024,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "\n",
        "model = TextGenerationModel.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def call_llm(model, parameters, llm_call, show_activity = True):\n",
        "  response = model.predict(llm_call, **parameters).text\n",
        "\n",
        "  if show_activity:\n",
        "    BOLD = \"\\033[1m\"\n",
        "    UNFORMAT = \"\\033[0m\\x1B[0m\"\n",
        "    print(f\"{BOLD}The call to the LLM:{UNFORMAT}\\n{llm_call}\\n\")\n",
        "    print(f\"{BOLD}The response:{UNFORMAT}\")\n",
        "    print(response)\n",
        "\n",
        "  return response  # Return to `_` if not needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "aD5WUUvDuHuD",
        "outputId": "40c6dc6a-bfd1-44fc-cbeb-f46e3a8da715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mThe call to the LLM:\u001b[0m\u001b[0m\n",
            "Who is President of Argentina?\n",
            "\n",
            "\u001b[1mThe response:\u001b[0m\u001b[0m\n",
            " Alberto Fernández\n"
          ]
        }
      ],
      "source": [
        "question = \"Who is President of Argentina?\" # Quien es el presidente de Argentina no va la wikitool\n",
        "_ = call_llm(model, parameters, question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91FoLDpruqF4"
      },
      "source": [
        "### The Wikipedia Tool\n",
        "\n",
        "The function below takes a query, returns the top Wikipedia article match for the query, and then retrieves the first `return_chars` characters of the article.\n",
        "\n",
        "This tool is for teaching purposes and is somewhat limited. It cannot access lists or sidebars, does not handle suggestions well, does not support search within a Wikipedia article, and may not always return a result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2cLj2TiCt0cn",
        "outputId": "7a972a36-8fd2-499e-8962-85a30000fe6f"
      },
      "outputs": [],
      "source": [
        "import wikipedia\n",
        "def wiki_tool(query, return_chars = 1000):\n",
        "  try:\n",
        "    page = wikipedia.page(query, auto_suggest=False, redirect=True).content\n",
        "  # If no exact match, take Wikipedia's auto-suggestion.\n",
        "  except wikipedia.exceptions.PageError as e:\n",
        "    page = wikipedia.page(query, auto_suggest=True, redirect=True).content\n",
        "  snippet = page[0:return_chars]\n",
        "  return snippet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRZ6v1z0uWAd"
      },
      "source": [
        "Try the tool:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "A4o-3Td9uZ-U",
        "outputId": "b9bdaec7-4d2a-401c-bf16-7661ceb327c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The president of Argentina (Spanish: Presidente de Argentina; officially known as the president of the Argentine Nation and president of the Argentine Republic, Spanish: Presidente de la Nación Argentina) is both head of state and head of government of Argentina. Under the national constitution, the president is also the chief executive of the federal government and commander-in-chief of the armed forces.\\nThroughout Argentine history, the office of head of state has undergone many changes, both in its title as in its features and powers. The current president Javier Milei was sworn into office on 10 December 2023. He succeeded Alberto Fernández. \\nThe constitution of Argentina, along with several constitutional amendments, establishes the requirements, powers, and responsibilities of the president, the term of office and the method of election.\\n\\n\\n== History ==\\nThe origins of Argentina as a nation can be traced to 1776, when it was separated by the King Charles III of Spain from the exis'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wiki_tool(\"president  of argentina\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7gGYXbxs8b7"
      },
      "source": [
        "### Chaining LLM Calls for Tool Use\n",
        "\n",
        "A basic two-step tool use LLM chain contains a few pieces, broken down here step-by-step.\n",
        "\n",
        "If you call the model (as of October 2023) with this example question about an obscure musician it hallucinates an incorrect answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "cHK1aJ_oXtJZ",
        "outputId": "66111769-f501-48fc-9160-c6911384310b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mThe call to the LLM:\u001b[0m\u001b[0m\n",
            "Quien publicó el album Struggle for Pleasure ?\n",
            "\n",
            "\u001b[1mThe response:\u001b[0m\u001b[0m\n",
            " El álbum Struggle for Pleasure fue publicado por la banda de rock estadounidense The Replacements en 1989.\n"
          ]
        }
      ],
      "source": [
        "question = \"Quien publicó el album Struggle for Pleasure ?\"\n",
        "_ = call_llm(model, parameters, question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nqc0vhU5H1X"
      },
      "source": [
        "### Step 1: Provide the LLM Instructions for Using the Tool\n",
        "\n",
        "You must provide the LLM both instructions for your task and for how to use the tool.\n",
        "\n",
        "This \"instructions\" part of the LLM call is sometimes called the \"context\" or some variation of \"condition\" (\"conditioning\", \"conditioning prompt\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "rhWpoRFGA21n",
        "outputId": "07d6e495-39ea-493a-bfa1-5e8559f7d89b"
      },
      "outputs": [],
      "source": [
        "context = \"\"\"Answer questions using a lookup of Wikipedia.\n",
        "After each question, write a Wikipedia search followed by '<STOP>'.\n",
        "The Wikipedia search will be used to retrieve the most relevant content.\n",
        "A section of the Wikipedia article will then be sent to the next LLM call.\n",
        "Use the text of the Wikipedia article to answer the question.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqWY6f3EBDyO"
      },
      "source": [
        "### Step 2: Provide An Exemplar\n",
        "\n",
        "The LLM needs exemplars that show how to use the tool to complete the task.\n",
        "\n",
        "This example has only a one-shot exemplar, few-shot would be better.\n",
        "\n",
        "The Wikipedia article text in this exemplar comes from running `wiki_tool(\"chancellor of germany\")` in August 2023.\n",
        "\n",
        "Note: After future retrainings the LLM will answer this question correctly without an external tool. But this one-shot exemplar will still work, since it shows the pattern of a Wikipedia search, a response, and an answer based on the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Haoj8nWSA_fy",
        "outputId": "822063b0-81f0-4796-d2fa-89f566a49293"
      },
      "outputs": [],
      "source": [
        "exemplar = \"\"\"Question: Who is Chancellor of Germany?\n",
        "Wikipedia Search: chancellor of Germany<STOP>\n",
        "Wikipedia Article: The chancellor of Germany, officially the federal chancellor of the Federal Republic of Germany, is the head of the federal government of Germany, and the commander in chief of the German Armed Forces during wartime. The chancellor is the chief executive of the Federal Cabinet and heads the executive branch. The chancellor is elected by the Bundestag on the proposal of the federal president and without debate (Article 63 of the German Constitution).The current officeholder is Olaf Scholz of the SPD, who was elected in December 2021, succeeding Angela Merkel. He was elected after the SPD entered into a coalition agreement with Alliance 90/The Greens and the FDP.\\n\\n\\n== History of the office ==\\nThe office of Chancellor has a long history, stemming back to the Holy Roman Empire, when the office of German archchancellor was usually held by archbishops of Mainz. The title was, at times, used in several states of German-speaking Europe. The modern office of chancellor was established with the\n",
        "Answer: Olaf Scholz\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deMaQ9ddDQhc"
      },
      "source": [
        "### Step 3: Make the First Call in the LLM Chain\n",
        "\n",
        "We'll combine our context and our exemplar together with our question and make a call to the LLM asking for a Wikipedia search query as a response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PC4l5oHtD9OO",
        "outputId": "7da83524-a7ee-4374-b7ad-a41a60ad282e"
      },
      "outputs": [],
      "source": [
        "step_one_call = f\"\"\"{context}\n",
        "\n",
        "{exemplar}\n",
        "\n",
        "Question: {question}\n",
        "Wikipedia Search:\"\"\"\n",
        "step_one_response = call_llm(model, parameters, step_one_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDUi5JB8GCL9"
      },
      "source": [
        "### Step 4: Use the LLM's Response to Query the Tool\n",
        "\n",
        "Note the LLM response contains more than the Wikipedia search query.\n",
        "\n",
        "LLMs work by repeatedly predicting the next token over and over again, based on the tokens in the LLM call plus any previously predicted tokens. This means the LLM will generate excess text, it does not know to stop after the Wikipedia search query.\n",
        "\n",
        "Everything beyond the Wikipedia search query is garbage. The excess text is discarded using the `<STOP>` signifier, though this could also be done with line breaks.\n",
        "\n",
        "In a production system, it's important to control costs by limiting the response size when making an LLM call like this.\n",
        "\n",
        "The following function takes the LLM response from the first chain step and returns the Wikipedia query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_2cqh5R4HTHV",
        "outputId": "171799dc-f330-4389-8aa9-18fedf17089c"
      },
      "outputs": [],
      "source": [
        "def get_wiki_query (llm_response, stop_text = \"<STOP>\"):\n",
        "  # Assumes the query is in the first line.\n",
        "  first_line = llm_response.splitlines()[0]\n",
        "  query = first_line.split(stop_text)[0]\n",
        "  return query.strip() # Remove leading and trailing whitespace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sv6ox89JYPe"
      },
      "source": [
        "Use this function on the response from the previous LLM call to extract the query, then  use `wiki_tool` to search Wikipedia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "0d5CKJRyJW5C",
        "outputId": "03d81998-97cd-4332-dfb0-c2666b4f188a"
      },
      "outputs": [],
      "source": [
        "wiki_query = get_wiki_query(step_one_response)\n",
        "print(f\"Tool Query: {wiki_query}\")\n",
        "wiki_text = wiki_tool(wiki_query)\n",
        "print(f\"Wikipedia Snippet: {wiki_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAmH5sQddF9Q"
      },
      "source": [
        "### Step 5: Use the Tool Response to Make the Second Call in the LLM Chain\n",
        "\n",
        "Next, answer the question by taking the output from the tool and constructing a second LLM call.\n",
        "\n",
        "LLM tool usage generally maintains the history of the previous calls and responses. To construct the second call in the chain:\n",
        "1. Start with the first LLM call in the chain.\n",
        "1. Append the previously generated Wikipedia query.\n",
        "1. Append the Wikipedia search result.\n",
        "\n",
        "Here's a reminder of what our first call looked like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "UJKx_TKAdmRz",
        "outputId": "97b41a40-de62-42f1-fa8c-014f27faf891"
      },
      "outputs": [],
      "source": [
        "print(step_one_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCCGiIZuChoA"
      },
      "source": [
        "This first LLM call is combined with the query from the first LLM response and the output from the Wikipedia tool, along with structure to match the exemplar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "gRsLkHfRd3hY",
        "outputId": "41b76607-4dcd-4455-a3cb-b9e215e74c9d"
      },
      "outputs": [],
      "source": [
        "step_two_call = f\"\"\"{step_one_call} {wiki_query}\n",
        "Wikipedia Article: {wiki_text}\n",
        "Answer: \"\"\"\n",
        "step_two_response = call_llm(model, parameters, step_two_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU-UI3mnKPLq"
      },
      "source": [
        "## Putting All the Steps Together\n",
        "\n",
        "This code snippet below gathers all the steps above, dependent packages, and dependent functions into a single function that manages the two-step tool usage LLM chain.\n",
        "\n",
        "You can copy and paste this code into your own project and it should work, assuming you've installed the right packages and authenticated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "o__JbR9LKiNX",
        "outputId": "a13b0ff0-b8c2-4f9a-d0af-24adb61e2fcb"
      },
      "outputs": [],
      "source": [
        "import wikipedia\n",
        "\n",
        "def call_llm(model, parameters, llm_call, show_activity = True):\n",
        "  # Wraps an LLM call to Vertex, optionally displaying the call and response.\n",
        "  response = model.predict(llm_call, **parameters).text\n",
        "\n",
        "  if show_activity:\n",
        "    BOLD = \"\\033[1m\"\n",
        "    UNFORMAT = \"\\033[0m\\x1B[0m\"\n",
        "    print(f\"{BOLD}The call to the LLM:{UNFORMAT}\\n{llm_call}\\n\")\n",
        "    print(f\"{BOLD}The response:{UNFORMAT}\")\n",
        "    print(response)\n",
        "\n",
        "  return response  # Return to `_` if not needed.\n",
        "\n",
        "\n",
        "def wiki_tool(query, return_chars = 1000):\n",
        "  try:\n",
        "    page = wikipedia.page(query, auto_suggest=False, redirect=True).content\n",
        "  # If no exact match, take Wikipedia's suggestion.\n",
        "  except wikipedia.exceptions.PageError as e:\n",
        "    page = wikipedia.page(query, auto_suggest=True, redirect=True).content\n",
        "  snippet = page[0:return_chars]\n",
        "  return snippet\n",
        "\n",
        "\n",
        "def get_wiki_query (llm_response, stop_text = \"<STOP>\"):\n",
        "  # Extract the wikipedia query from the LLM response.\n",
        "  # Assumes the query is in the first line.\n",
        "  first_line = llm_response.splitlines()[0]\n",
        "  query = first_line.split(stop_text)[0]\n",
        "  return query.strip() # Remove leading and trailing whitespace\n",
        "\n",
        "\n",
        "def wiki_tool_chain(model,\n",
        "                    parameters,\n",
        "                    context,\n",
        "                    exemplar,\n",
        "                    question,\n",
        "                    show_activity=False):\n",
        "  # Answer a query using wikipedia by calling an LLM.\n",
        "  step_one_call = (\n",
        "      f\"{context}\\n\\n{exemplar}\\n\\nQuestion: {question}\\nWikipedia Search:\"\n",
        "  )\n",
        "  if show_activity:\n",
        "    print(\"\\033[1mMaking the first LLM call...\\033[0m\\x1B[0m\")\n",
        "  step_one_response = call_llm(model, parameters, step_one_call, show_activity)\n",
        "  wiki_query = get_wiki_query(step_one_response)\n",
        "  wiki_text = wiki_tool(wiki_query)\n",
        "\n",
        "  step_two_call = (\n",
        "      f\"{step_one_call} {wiki_query}\\nWikipedia Article: {wiki_text}\\nAnswer: \"\n",
        "  )\n",
        "  if show_activity:\n",
        "    print(\"\\033[1mMaking the second LLM call...\\033[0m\\x1B[0m\")\n",
        "  step_two_response = call_llm(model, parameters, step_two_call, show_activity)\n",
        "\n",
        "  return step_two_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l9ChpYxWlS3"
      },
      "source": [
        "An example using the code above:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ChHBEqg7MQCZ",
        "outputId": "2d309990-d585-4343-a0b8-0649782fb335"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "# Outside this notebook, set PROJECT_ID, LOCATION, and MODEL_NAME.\n",
        "# When running in the notebook, these are set in part 0.\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "# These settings control how deterministic the LLM response is.\n",
        "parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "context = \"\"\"Answer questions using a lookup of wikipedia.\n",
        "After each question, write a wikipedia search followed by '<STOP>'.\n",
        "The wikipedia search will be used to retrieve the most relevant content.\n",
        "A section of the wikipedia article will then be sent to the next LLM call.\n",
        "Use the text of the wikipedia article to answer the question.\"\"\"\n",
        "\n",
        "exemplar = \"\"\"Question: Who is Chancellor of Germany?\n",
        "Wikipedia Search: chancellor of Germany<STOP>\n",
        "Wikipedia Article: The chancellor of Germany, officially the federal chancellor of the Federal Republic of Germany, is the head of the federal government of Germany, and the commander in chief of the German Armed Forces during wartime. The chancellor is the chief executive of the Federal Cabinet and heads the executive branch. The chancellor is elected by the Bundestag on the proposal of the federal president and without debate (Article 63 of the German Constitution).The current officeholder is Olaf Scholz of the SPD, who was elected in December 2021, succeeding Angela Merkel. He was elected after the SPD entered into a coalition agreement with Alliance 90/The Greens and the FDP.\\n\\n\\n== History of the office ==\\nThe office of Chancellor has a long history, stemming back to the Holy Roman Empire, when the office of German archchancellor was usually held by archbishops of Mainz. The title was, at times, used in several states of German-speaking Europe. The modern office of chancellor was established with the\n",
        "Answer: Olaf Scholz\"\"\"\n",
        "\n",
        "answer = wiki_tool_chain(model,\n",
        "                         parameters,\n",
        "                         context,\n",
        "                         exemplar,\n",
        "                         \"Quien publicó el album Struggle for pleasure?\",\n",
        "                         show_activity = False)\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oukbFAyxoNPR"
      },
      "source": [
        "With `show_activity = True` to see the breakdown of the LLM calls:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uU3h3GkcbgUn",
        "outputId": "2959234c-d6a6-4fec-ea4e-6fae613eb4f4"
      },
      "outputs": [],
      "source": [
        "wiki_tool_chain(model,\n",
        "                parameters,\n",
        "                context,\n",
        "                exemplar,\n",
        "                \"Quien publicó el album Struggle for pleasure?\",\n",
        "                show_activity = True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
