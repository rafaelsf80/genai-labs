{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ5caKL2Ff2B"
      },
      "source": [
        "# 01-2: External tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai.types import GenerateContentConfig\n",
        "\n",
        "import getpass\n",
        "google_api_key = getpass.getpass() \n",
        "\n",
        "google_client = genai.Client(api_key=google_api_key)\n",
        "\n",
        "parameters = dict(\n",
        "    temperature = 1.0,\n",
        "    max_output_tokens = 128,\n",
        "    top_p = 0.8,\n",
        "    top_k = 40,\n",
        ")\n",
        "\n",
        "MODEL_GEMMA = \"gemma-3-1b-it\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def call_llm(model, config, llm_call, show_activity = True):\n",
        "  \n",
        "  response = google_client.models.generate_content(\n",
        "        model=model,\n",
        "        contents=llm_call,\n",
        "        config=GenerateContentConfig(\n",
        "            temperature=config[\"temperature\"],\n",
        "            top_p=config[\"top_p\"],\n",
        "            top_k=config[\"top_k\"],\n",
        "            candidate_count=1,  \n",
        "    )\n",
        "  )\n",
        "\n",
        "  if show_activity:\n",
        "    BOLD = \"\\033[1m\"\n",
        "    UNFORMAT = \"\\033[0m\\x1B[0m\"\n",
        "    print(f\"{BOLD}The call to the LLM:{UNFORMAT}\\n{llm_call}\\n\")\n",
        "    print(f\"{BOLD}The response:{UNFORMAT}\")\n",
        "    print(response.text)\n",
        "\n",
        "  return response.text  # Return to `_` if not needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "aD5WUUvDuHuD",
        "outputId": "40c6dc6a-bfd1-44fc-cbeb-f46e3a8da715"
      },
      "outputs": [],
      "source": [
        "question = \"Who is Chancellor of Germany?\" \n",
        "_ = call_llm(MODEL_GEMMA, parameters, question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91FoLDpruqF4"
      },
      "source": [
        "### The Wikipedia Tool\n",
        "\n",
        "The function below takes a query, returns the top Wikipedia article match for the query, and then retrieves the first `return_chars` characters of the article.\n",
        "\n",
        "This tool is for teaching purposes and is somewhat limited. It cannot access lists or sidebars, does not handle suggestions well, does not support search within a Wikipedia article, and may not always return a result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2cLj2TiCt0cn",
        "outputId": "7a972a36-8fd2-499e-8962-85a30000fe6f"
      },
      "outputs": [],
      "source": [
        "import wikipedia\n",
        "def wiki_tool(query, return_chars = 1000):\n",
        "  try:\n",
        "    page = wikipedia.page(query, auto_suggest=False, redirect=True).content\n",
        "  # If no exact match, take Wikipedia's auto-suggestion.\n",
        "  except wikipedia.exceptions.PageError as e:\n",
        "    page = wikipedia.page(query, auto_suggest=True, redirect=True).content\n",
        "  snippet = page[0:return_chars]\n",
        "  return snippet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRZ6v1z0uWAd"
      },
      "source": [
        "Try the tool:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "A4o-3Td9uZ-U",
        "outputId": "b9bdaec7-4d2a-401c-bf16-7661ceb327c0"
      },
      "outputs": [],
      "source": [
        "wiki_tool(\"chancellor of germany\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7gGYXbxs8b7"
      },
      "source": [
        "### Chaining LLM Calls for Tool Use\n",
        "\n",
        "A basic two-step tool use LLM chain contains a few pieces, broken down here step-by-step.\n",
        "\n",
        "If you call the model (as of October 2023) with this example question about an obscure musician it hallucinates an incorrect answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "cHK1aJ_oXtJZ",
        "outputId": "66111769-f501-48fc-9160-c6911384310b"
      },
      "outputs": [],
      "source": [
        "question = \"Who is Chancellor of Germany?\"\n",
        "_ = call_llm(MODEL_GEMMA, parameters, question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nqc0vhU5H1X"
      },
      "source": [
        "### Step 1: Provide the LLM Instructions for Using the Tool\n",
        "\n",
        "You must provide the LLM both instructions for your task and for how to use the tool.\n",
        "\n",
        "This \"instructions\" part of the LLM call is sometimes called the \"context\" or some variation of \"condition\" (\"conditioning\", \"conditioning prompt\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "rhWpoRFGA21n",
        "outputId": "07d6e495-39ea-493a-bfa1-5e8559f7d89b"
      },
      "outputs": [],
      "source": [
        "context = \"\"\"Answer questions using a lookup of Wikipedia.\n",
        "After each question, write a Wikipedia search followed by '<STOP>'.\n",
        "The Wikipedia search will be used to retrieve the most relevant content.\n",
        "A section of the Wikipedia article will then be sent to the next LLM call.\n",
        "Use the text of the Wikipedia article to answer the question.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqWY6f3EBDyO"
      },
      "source": [
        "### Step 2: Provide An Exemplar\n",
        "\n",
        "The LLM needs exemplars that show how to use the tool to complete the task.\n",
        "\n",
        "This example has only a one-shot exemplar, few-shot would be better.\n",
        "\n",
        "The Wikipedia article text in this exemplar comes from running `wiki_tool(\"chancellor of germany\")` in August 2023.\n",
        "\n",
        "Note: After future retrainings the LLM will answer this question correctly without an external tool. But this one-shot exemplar will still work, since it shows the pattern of a Wikipedia search, a response, and an answer based on the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Haoj8nWSA_fy",
        "outputId": "822063b0-81f0-4796-d2fa-89f566a49293"
      },
      "outputs": [],
      "source": [
        "exemplar = \"\"\"Question: Who is Chancellor of Germany?\n",
        "Wikipedia Search: chancellor of Germany<STOP>\n",
        "Wikipedia Article: The chancellor of Germany, officially the federal chancellor of the Federal Republic of Germany, is the head of the federal government of Germany. The chancellor is the chief executive of the Federal Cabinet and heads the executive branch. The chancellor is elected by the Bundestag on the proposal of the federal president and without debate (Article 63 of the German Constitution). During a state of defence declared by the Bundestag the chancellor also assumes the position of commander-in-chief of the Bundeswehr.\\nTen people (nine men and one woman) have served as chancellor of the Federal Republic of Germany, the first being Konrad Adenauer from 1949 to 1963. (Another 26 men had served as \"Reich chancellors\" of the previous German Empire from 1871 to 1945.) The current officeholder is Friedrich Merz of the Christian Democratic Union, sworn in on 6 May 2025.\\n\\n\\n== History of the office (pre-1949) ==\\n\\nThe office of chancellor has a long history, stemming back to the Holy Roman Empire (c. 9\n",
        "Answer: Friedrich Merz\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deMaQ9ddDQhc"
      },
      "source": [
        "### Step 3: Make the First Call in the LLM Chain\n",
        "\n",
        "We'll combine our context and our exemplar together with our question and make a call to the LLM asking for a Wikipedia search query as a response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PC4l5oHtD9OO",
        "outputId": "7da83524-a7ee-4374-b7ad-a41a60ad282e"
      },
      "outputs": [],
      "source": [
        "step_one_call = f\"\"\"{context}\n",
        "\n",
        "{exemplar}\n",
        "\n",
        "Question: {question}\n",
        "Wikipedia Search:\"\"\"\n",
        "step_one_response = call_llm(MODEL_GEMMA, parameters, step_one_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDUi5JB8GCL9"
      },
      "source": [
        "### Step 4: Use the LLM's Response to Query the Tool\n",
        "\n",
        "Note the LLM response contains more than the Wikipedia search query.\n",
        "\n",
        "LLMs work by repeatedly predicting the next token over and over again, based on the tokens in the LLM call plus any previously predicted tokens. This means the LLM will generate excess text, it does not know to stop after the Wikipedia search query.\n",
        "\n",
        "Everything beyond the Wikipedia search query is garbage. The excess text is discarded using the `<STOP>` signifier, though this could also be done with line breaks.\n",
        "\n",
        "In a production system, it's important to control costs by limiting the response size when making an LLM call like this.\n",
        "\n",
        "The following function takes the LLM response from the first chain step and returns the Wikipedia query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_2cqh5R4HTHV",
        "outputId": "171799dc-f330-4389-8aa9-18fedf17089c"
      },
      "outputs": [],
      "source": [
        "def get_wiki_query (llm_response, stop_text = \"<STOP>\"):\n",
        "  # Assumes the query is in the first line.\n",
        "  first_line = llm_response.splitlines()[0]\n",
        "  query = first_line.split(stop_text)[0]\n",
        "  return query.strip() # Remove leading and trailing whitespace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sv6ox89JYPe"
      },
      "source": [
        "Use this function on the response from the previous LLM call to extract the query, then  use `wiki_tool` to search Wikipedia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "0d5CKJRyJW5C",
        "outputId": "03d81998-97cd-4332-dfb0-c2666b4f188a"
      },
      "outputs": [],
      "source": [
        "wiki_query = get_wiki_query(step_one_response)\n",
        "print(f\"Tool Query: {wiki_query}\")\n",
        "wiki_text = wiki_tool(wiki_query)\n",
        "print(f\"Wikipedia Snippet: {wiki_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAmH5sQddF9Q"
      },
      "source": [
        "### Step 5: Use the Tool Response to Make the Second Call in the LLM Chain\n",
        "\n",
        "Next, answer the question by taking the output from the tool and constructing a second LLM call.\n",
        "\n",
        "LLM tool usage generally maintains the history of the previous calls and responses. To construct the second call in the chain:\n",
        "1. Start with the first LLM call in the chain.\n",
        "1. Append the previously generated Wikipedia query.\n",
        "1. Append the Wikipedia search result.\n",
        "\n",
        "Here's a reminder of what our first call looked like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "UJKx_TKAdmRz",
        "outputId": "97b41a40-de62-42f1-fa8c-014f27faf891"
      },
      "outputs": [],
      "source": [
        "print(step_one_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCCGiIZuChoA"
      },
      "source": [
        "This first LLM call is combined with the query from the first LLM response and the output from the Wikipedia tool, along with structure to match the exemplar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "gRsLkHfRd3hY",
        "outputId": "41b76607-4dcd-4455-a3cb-b9e215e74c9d"
      },
      "outputs": [],
      "source": [
        "step_two_call = f\"\"\"{step_one_call} {wiki_query}\n",
        "Wikipedia Article: {wiki_text}\n",
        "Answer: \"\"\"\n",
        "step_two_response = call_llm(MODEL_GEMMA, parameters, step_two_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU-UI3mnKPLq"
      },
      "source": [
        "## Putting All the Steps Together\n",
        "\n",
        "This code snippet below gathers all the steps above, dependent packages, and dependent functions into a single function that manages the two-step tool usage LLM chain.\n",
        "\n",
        "You can copy and paste this code into your own project and it should work, assuming you've installed the right packages and authenticated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "o__JbR9LKiNX",
        "outputId": "a13b0ff0-b8c2-4f9a-d0af-24adb61e2fcb"
      },
      "outputs": [],
      "source": [
        "import wikipedia\n",
        "\n",
        "def call_llm(model, config, llm_call, show_activity = True):\n",
        "  \n",
        "  response = google_client.models.generate_content(\n",
        "        model=model,\n",
        "        contents=llm_call,\n",
        "        config=GenerateContentConfig(\n",
        "            temperature=config[\"temperature\"],\n",
        "            top_p=config[\"top_p\"],\n",
        "            top_k=config[\"top_k\"],\n",
        "            candidate_count=1,  \n",
        "    )\n",
        "  )\n",
        "\n",
        "  if show_activity:\n",
        "    BOLD = \"\\033[1m\"\n",
        "    UNFORMAT = \"\\033[0m\\x1B[0m\"\n",
        "    print(f\"{BOLD}The call to the LLM:{UNFORMAT}\\n{llm_call}\\n\")\n",
        "    print(f\"{BOLD}The response:{UNFORMAT}\")\n",
        "    print(response.text)\n",
        "\n",
        "  return response.text  # Return to `_` if not needed.\n",
        "\n",
        "\n",
        "def wiki_tool(query, return_chars = 1000):\n",
        "  try:\n",
        "    page = wikipedia.page(query, auto_suggest=False, redirect=True).content\n",
        "  # If no exact match, take Wikipedia's suggestion.\n",
        "  except wikipedia.exceptions.PageError as e:\n",
        "    page = wikipedia.page(query, auto_suggest=True, redirect=True).content\n",
        "  snippet = page[0:return_chars]\n",
        "  return snippet\n",
        "\n",
        "\n",
        "def get_wiki_query (llm_response, stop_text = \"<STOP>\"):\n",
        "  # Extract the wikipedia query from the LLM response.\n",
        "  # Assumes the query is in the first line.\n",
        "  first_line = llm_response.splitlines()[0]\n",
        "  query = first_line.split(stop_text)[0]\n",
        "  return query.strip() # Remove leading and trailing whitespace\n",
        "\n",
        "\n",
        "def wiki_tool_chain(model,\n",
        "                    parameters,\n",
        "                    context,\n",
        "                    exemplar,\n",
        "                    question,\n",
        "                    show_activity=False):\n",
        "  # Answer a query using wikipedia by calling an LLM.\n",
        "  step_one_call = (\n",
        "      f\"{context}\\n\\n{exemplar}\\n\\nQuestion: {question}\\nWikipedia Search:\"\n",
        "  )\n",
        "  if show_activity:\n",
        "    print(\"\\033[1mMaking the first LLM call...\\033[0m\\x1B[0m\")\n",
        "  step_one_response = call_llm(model, parameters, step_one_call, show_activity)\n",
        "  wiki_query = get_wiki_query(step_one_response)\n",
        "  wiki_text = wiki_tool(wiki_query)\n",
        "\n",
        "  step_two_call = (\n",
        "      f\"{step_one_call} {wiki_query}\\nWikipedia Article: {wiki_text}\\nAnswer: \"\n",
        "  )\n",
        "  if show_activity:\n",
        "    print(\"\\033[1mMaking the second LLM call...\\033[0m\\x1B[0m\")\n",
        "  step_two_response = call_llm(model, parameters, step_two_call, show_activity)\n",
        "\n",
        "  return step_two_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l9ChpYxWlS3"
      },
      "source": [
        "An example using the code above:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ChHBEqg7MQCZ",
        "outputId": "2d309990-d585-4343-a0b8-0649782fb335"
      },
      "outputs": [],
      "source": [
        "parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "\n",
        "context = \"\"\"Answer questions using a lookup of wikipedia.\n",
        "After each question, write a wikipedia search followed by '<STOP>'.\n",
        "The wikipedia search will be used to retrieve the most relevant content.\n",
        "A section of the wikipedia article will then be sent to the next LLM call.\n",
        "Use the text of the wikipedia article to answer the question.\"\"\"\n",
        "\n",
        "exemplar = \"\"\"Question: Who is Chancellor of Germany?\n",
        "Wikipedia Search: chancellor of Germany<STOP>\n",
        "Wikipedia Article: The chancellor of Germany, officially the federal chancellor of the Federal Republic of Germany, is the head of the federal government of Germany. The chancellor is the chief executive of the Federal Cabinet and heads the executive branch. The chancellor is elected by the Bundestag on the proposal of the federal president and without debate (Article 63 of the German Constitution). During a state of defence declared by the Bundestag the chancellor also assumes the position of commander-in-chief of the Bundeswehr.\\nTen people (nine men and one woman) have served as chancellor of the Federal Republic of Germany, the first being Konrad Adenauer from 1949 to 1963. (Another 26 men had served as \"Reich chancellors\" of the previous German Empire from 1871 to 1945.) The current officeholder is Friedrich Merz of the Christian Democratic Union, sworn in on 6 May 2025.\\n\\n\\n== History of the office (pre-1949) ==\\n\\nThe office of chancellor has a long history, stemming back to the Holy Roman Empire (c. 9\n",
        "Answer: Friedrich Merz\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "answer = wiki_tool_chain(MODEL_GEMMA,\n",
        "                         parameters,\n",
        "                         context,\n",
        "                         exemplar,\n",
        "                         #\"Quien publicó el album Struggle for pleasure?\",\n",
        "                        \"Who is Chancellor of Germany?\",\n",
        "                         show_activity = False)\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oukbFAyxoNPR"
      },
      "source": [
        "With `show_activity = True` to see the breakdown of the LLM calls:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uU3h3GkcbgUn",
        "outputId": "2959234c-d6a6-4fec-ea4e-6fae613eb4f4"
      },
      "outputs": [],
      "source": [
        "wiki_tool_chain(MODEL_GEMMA,\n",
        "                parameters,\n",
        "                context,\n",
        "                exemplar,\n",
        "                #\"Quien publicó el album Struggle for pleasure?\",\n",
        "                \"Who is Chancellor of Germany?\",\n",
        "                show_activity = True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
