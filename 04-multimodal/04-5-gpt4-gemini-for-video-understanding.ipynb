{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04-5: GPT-4 and Gemini for video and TTS API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image, Audio\n",
    "\n",
    "import cv2        # We're using OpenCV to read video\n",
    "import base64\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import getpass\n",
    "\n",
    "openai_api_key = getpass.getpass(\"OPENAI API Key:\")\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using OpenCV to extract frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we use OpenCV to extract frames from a nature [video](https://www.youtube.com/watch?v=kQ_7GtE529M) containing bisons and wolves:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture(\"./chicago2.mp4\")\n",
    "\n",
    "base64Frames = []\n",
    "while video.isOpened():\n",
    "    success, frame = video.read()\n",
    "    if not success:\n",
    "        break\n",
    "    _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "    base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "\n",
    "video.release()\n",
    "print(len(base64Frames), \"frames read.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display frames to make sure we've read them in correctly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_handle = display(None, display_id=True)\n",
    "for img in base64Frames:\n",
    "    display_handle.update(Image(data=base64.b64decode(img.encode(\"utf-8\"))))\n",
    "    time.sleep(0.025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using GPT's visual capabilities to get a description of a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "import getpass\n",
    "openai_api_key = getpass.getpass()\n",
    "\n",
    "openai_client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = openai_client.responses.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": (\n",
    "                        \"These are frames of a video. Create a short voiceover script in the style of David Attenborough. Only include the narration.\"\n",
    "                    )\n",
    "                },\n",
    "                *[\n",
    "                    {\n",
    "                        \"type\": \"input_image\",\n",
    "                        \"image_url\": f\"data:image/jpeg;base64,{frame}\"\n",
    "                    }\n",
    "                    for frame in base64Frames[0::25]\n",
    "                ]\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(result.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating a voiceover using TTS API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a voiceover for this video in the style of David Attenborough. Using the same video frames we prompt GPT to give us a short script:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "Voice Affect: Calm, measured, and warmly engaging; convey awe and quiet reverence for the natural world.\n",
    "\n",
    "Tone: Inquisitive and insightful, with a gentle sense of wonder and deep respect for the subject matter.\n",
    "\n",
    "Pacing: Even and steady, with slight lifts in rhythm when introducing a new species or unexpected behavior; natural pauses to allow the viewer to absorb visuals.\n",
    "\n",
    "Emotion: Subtly emotive—imbued with curiosity, empathy, and admiration without becoming sentimental or overly dramatic.\n",
    "\n",
    "Emphasis: Highlight scientific and descriptive language (“delicate wings shimmer in the sunlight,” “a symphony of unseen life,” “ancient rituals played out beneath the canopy”) to enrich imagery and understanding.\n",
    "\n",
    "Pronunciation: Clear and articulate, with precise enunciation and slightly rounded vowels to ensure accessibility and authority.\n",
    "\n",
    "Pauses: Insert thoughtful pauses before introducing key facts or transitions (“And then... with a sudden rustle...”), allowing space for anticipation and reflection.\n",
    "\"\"\"\n",
    "\n",
    "audio_response = response = openai_client.audio.speech.create(\n",
    "  model=\"gpt-4o-mini-tts\",\n",
    "  voice=\"echo\",\n",
    "  instructions=instructions,\n",
    "  input=result.output_text,\n",
    "  response_format=\"wav\"\n",
    ")\n",
    "\n",
    "audio_bytes = audio_response.content\n",
    "Audio(data=audio_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gemini 2.5 Flash for video summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "import getpass\n",
    "google_api_key = getpass.getpass()\n",
    "\n",
    "client = genai.Client(api_key=google_api_key)\n",
    "\n",
    "MODEL_ID = \"gemini-2.5-flash-preview-05-20\" \n",
    "PROMPT = \"Por favor, haz un resumen de este video en 3 frases en español.\"\n",
    "\n",
    "import time\n",
    "\n",
    "def upload_video(video_file_name):\n",
    "  video_file = client.files.upload(file=video_file_name)\n",
    "\n",
    "  while video_file.state == \"PROCESSING\":\n",
    "      print('Waiting for video to be processed.')\n",
    "      time.sleep(10)\n",
    "      video_file = client.files.get(name=video_file.name)\n",
    "\n",
    "  if video_file.state == \"FAILED\":\n",
    "    raise ValueError(video_file.state)\n",
    "  print(f'Video processing complete: ' + video_file.uri)\n",
    "\n",
    "  return video_file\n",
    "\n",
    "chicago_video = upload_video(\"./chicago2.mp4\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        chicago_video,\n",
    "        PROMPT,\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TTS with Chirp3-HD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade --quiet google-cloud-texttospeech\n",
    "from google.cloud import texttospeech_v1beta1 as texttospeech\n",
    "from google.api_core.client_options import ClientOptions\n",
    "\n",
    "\n",
    "prompt = response.text\n",
    "\n",
    "voice = \"Aoede\"  # @param [\"Aoede\", \"Puck\", \"Charon\", \"Kore\", \"Fenrir\", \"Leda\", \"Orus\", \"Zephyr\"]\n",
    "\n",
    "language_code = \"es-ES\"  # @param [ \"de-DE\", \"en-AU\", \"en-GB\", \"en-IN\", \"en-US\", \"fr-FR\", \"hi-IN\", \"pt-BR\", \"ar-XA\", \"es-ES\", \"fr-CA\", \"id-ID\", \"it-IT\", \"ja-JP\", \"tr-TR\", \"vi-VN\", \"bn-IN\", \"gu-IN\", \"kn-IN\", \"ml-IN\", \"mr-IN\", \"ta-IN\", \"te-IN\", \"nl-NL\", \"ko-KR\", \"cmn-CN\", \"pl-PL\", \"ru-RU\", \"th-TH\"]\n",
    "\n",
    "voice_name = f\"{language_code}-Chirp3-HD-{voice}\"\n",
    "voice = texttospeech.VoiceSelectionParams(\n",
    "    name=voice_name,\n",
    "    language_code=language_code,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTS_LOCATION=\"global\"\n",
    "API_ENDPOINT = (\n",
    "    f\"{TTS_LOCATION}-texttospeech.googleapis.com\"\n",
    "    if TTS_LOCATION != \"global\"\n",
    "    else \"texttospeech.googleapis.com\"\n",
    ")\n",
    "\n",
    "client = texttospeech.TextToSpeechClient(\n",
    "    client_options=ClientOptions(api_endpoint=API_ENDPOINT)\n",
    ")\n",
    "\n",
    "response = client.synthesize_speech(\n",
    "    input=texttospeech.SynthesisInput(text=prompt),\n",
    "    voice=voice,\n",
    "    # Select the type of audio file you want returned\n",
    "    audio_config=texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "display(Audio(response.audio_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Veo3\n",
    "\n",
    "* Este ejemplo genera video con audio.\n",
    "* Veo3 tiene un coste de $0.75 / segundo de video, y no está incluido en el Free Tier.\n",
    "* Todos los videos de Veo3 llevan SynthID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "operation = client.models.generate_videos(\n",
    "    model=\"veo-3.0-generate-preview\",\n",
    "    prompt=\"Panning wide shot of a purring kitten sleeping in the sunshine\",\n",
    "    config=types.GenerateVideosConfig(\n",
    "        #person_generation=\"allow_all\",  # \"allow_adult\" and \"dont_allow\" for Veo 2 only\n",
    "        aspect_ratio=\"16:9\",  # \"16:9\", and \"9:16\" for Veo 2 only\n",
    "    ),\n",
    ")\n",
    "\n",
    "while not operation.done:\n",
    "    time.sleep(20)\n",
    "    operation = client.operations.get(operation)\n",
    "\n",
    "for n, generated_video in enumerate(operation.response.generated_videos):\n",
    "    client.files.download(file=generated_video.video)\n",
    "    generated_video.video.save(f\"video{n}.mp4\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
