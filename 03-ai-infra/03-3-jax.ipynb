{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_PJXw3uzyT0"
      },
      "source": [
        "# 03-3: JAX intro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n",
        "\n",
        "JAX is much more than just a GPU-backed NumPy. It also comes with a few program transformations that are useful when writing numerical code. For now, there's three main ones:\n",
        "\n",
        " - `jit`, for speeding up your code\n",
        " - `grad`, for taking derivatives\n",
        " - `vmap`, for automatic vectorization or batching.\n",
        "\n",
        "With its updated version of [Autograd](https://github.com/hips/autograd), JAX can automatically differentiate native Python and NumPy code. It can differentiate through a large subset of Python’s features, including loops, ifs, recursion, and closures, and it can even take derivatives of derivatives of derivatives. It supports reverse-mode as well as forward-mode differentiation, and the two can be composed arbitrarily to any order.\n",
        "\n",
        "What’s new is that JAX uses [XLA](https://www.tensorflow.org/xla) to compile and run your NumPy code on accelerators, like GPUs and TPUs.\n",
        "Compilation happens under the hood by default, with library calls getting just-in-time compiled and executed. But JAX even lets you just-in-time compile your own Python functions into XLA-optimized kernels using a one-function API. Compilation and automatic differentiation can be composed arbitrarily, so you can express sophisticated algorithms and get maximal performance without having to leave Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install jax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_117sy0CGEU"
      },
      "source": [
        "# JAX is \"Numpy on GPUs/TPUs\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABao3OtA6GHk"
      },
      "source": [
        "You can use most numpy functions in JAX numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqUzvqF1B1TO",
        "outputId": "bcd5f6c3-8220-491e-9c24-45eeb6eb0e22"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "x = jnp.arange(10)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fLtgPUAn7mi",
        "outputId": "2e9bb282-f323-4797-adda-3f19efbfe68d"
      },
      "outputs": [],
      "source": [
        "y = np.arange(10)\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx8VofzzoHFH"
      },
      "source": [
        "Compare numpy and jnp speeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux8XM2ogObxR",
        "outputId": "d78c9bdd-548f-4cf6-ca49-7b0961fd8544"
      },
      "outputs": [],
      "source": [
        "long_vector_np = np.arange(int(1e7))\n",
        "%timeit np.dot(long_vector_np, long_vector_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "test": {
            "output": "ignore"
          }
        },
        "id": "mRvjVxoqo-Bi",
        "outputId": "a3cb85c3-c4af-458d-f3a8-bbff35c635c0"
      },
      "outputs": [],
      "source": [
        "long_vector = jnp.arange(int(1e7))\n",
        "%timeit jnp.dot(long_vector, long_vector).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Matrix multiplications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Matrix multiplications can run on GPU directly. Note `.block_until_ready` because [JAX uses asynchronous execution by default](https://jax.readthedocs.io/en/latest/async_dispatch.html). JAX NumPy functions work on regular NumPy arrays:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "size = 3000\n",
        "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
        "%timeit jnp.dot(x, x.T).block_until_ready()  # runs on the GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's slower because it has to transfer data to the GPU every time. You can ensure that an NDArray is backed by device memory using `device_put`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from jax import device_put\n",
        "\n",
        "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
        "x = device_put(x)\n",
        "%timeit jnp.dot(x, x.T).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output of `device_put` still acts like an NDArray, but it only copies values back to the CPU when they're needed for printing, plotting, saving to disk, branching, etc. The behavior of `device_put` is equivalent to the function `jit(lambda x: x)`, but it's faster.\n",
        "\n",
        "If you have a GPU (or TPU!) these calls run on the accelerator and have the potential to be much faster than on CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
        "%timeit np.dot(x, x.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using jit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "JAX runs transparently on the GPU (or CPU, if you don't have one, and TPU coming soon!). However, in the above example, JAX is dispatching kernels to the GPU one operation at a time. If we have a sequence of operations, we can use the `@jit` decorator to compile multiple operations together using [XLA](https://www.tensorflow.org/xla). Let's try that.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def selu(x, alpha=1.67, lmbda=1.05):\n",
        "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
        "\n",
        "x = jnp.arange(10**7)\n",
        "%timeit selu(x).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can speed it up with `@jit`, which will jit-compile the first time `selu` is called and will be cached thereafter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selu_jit = jit(selu)\n",
        "%timeit selu_jit(x).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkCpI-v0uQQO"
      },
      "source": [
        "\n",
        "\n",
        "## Using grad for derivatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuaGUVRUvbzQ"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "  return jnp.sum(x**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5VCnRD27L3b"
      },
      "source": [
        "If x is of length 3, we have"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMuU4TsC6bv7"
      },
      "source": [
        "$$f(x_{1}, x_{2}, x_{3}) = x_1^2 + x_2^2+x_3^3$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM-WtftF7tPC"
      },
      "source": [
        "$$\\nabla f= \\left(\\frac{df}{dx_1}, \\frac{df}{dx_2}, \\frac{df}{dx_3}\\right)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye0DLjJu8VHg"
      },
      "source": [
        "$$\\nabla f(x_1,x_2,x_3) = (2x_1, 2x_2, 2x_3)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKeorwJfvpeI",
        "outputId": "0de20587-bcda-48bd-ce43-446713a6bb38"
      },
      "outputs": [],
      "source": [
        "grad_f = jax.grad(f)\n",
        "\n",
        "x = jnp.asarray([1.0, 2.0, 3.0])\n",
        "\n",
        "print(f(x))\n",
        "\n",
        "print(grad_f(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfI8RUHw90iA"
      },
      "source": [
        "We can go on taking derivatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3NfaVu4yrQE",
        "outputId": "15989dfb-6bc1-425c-8673-85351b862a2f"
      },
      "outputs": [],
      "source": [
        "f = lambda x: x**2 -x + 3\n",
        "dfdx = jax.grad(f)\n",
        "d2fdx = jax.grad(dfdx)\n",
        "d3fdx = jax.grad(d2fdx)\n",
        "\n",
        "x= 0.0\n",
        "# f(x) = x**2 - x +3 = 3\n",
        "print(\"f(0)=\",f(x))\n",
        "# f'(x) = 2x - 1 = -1\n",
        "print(\"f'(0)\",dfdx(x))\n",
        "#f''(x) = 2\n",
        "print(\"f''(0)=\",d2fdx(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsF-bcdYAaIK"
      },
      "source": [
        "Jacobian and Hessian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EFd9fa2ApAz"
      },
      "source": [
        "$$f(x,y) = x^2 +y^2$$\n",
        "$$J(f) = \\left(\\frac{df}{dx},\\frac{df}{dy}\\right)$$\n",
        "$$Hess(f) = \\begin{bmatrix} \\frac{d^2f}{dx^2} \\frac{d^2f}{dxdy} \\\\\n",
        "\\frac{d^2f}{dydx} \\frac{d^2f}{dy^2}\n",
        "\\end{bmatrix}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faoRKw4WAZha"
      },
      "outputs": [],
      "source": [
        "f = lambda x,y: x**2+y**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX26i2VfAm4b",
        "outputId": "729fb1d0-8c1f-4312-a3f3-f4bb38812099"
      },
      "outputs": [],
      "source": [
        "from jax import jacfwd, jacrev, jit\n",
        "jac = jacrev(f,argnums=(0,1))\n",
        "hessian = jit(jacfwd(jacrev(f,argnums=(0,1)),argnums=(0,1)))\n",
        "print(\"Jacobian:\",jac(1.0,1.0))\n",
        "print(\"Hessian:\",hessian(1.0,1.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-5TLCFaX1yP"
      },
      "source": [
        "## Should I jit all my functions ?? Jit has some limitations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnoLAPZ5Yyt4"
      },
      "source": [
        "JAX is designed for \"pure functions\", meaning:\n",
        "- all the input data is passed through the function parameters, and all the results are output through the function results\n",
        "- deterministic, the same result is returned when calling a function with the same inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3kqp6JbpTOe"
      },
      "source": [
        "Example of \"unpure\" functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG2i0XZJpakZ",
        "outputId": "df865112-d169-4b5a-b9e1-e1d964d1ccac"
      },
      "outputs": [],
      "source": [
        "# Example 1\n",
        "\n",
        "def impure_print_side_effect(x):\n",
        "    print(\"Executing function\")  # Violating #1\n",
        "    return x\n",
        "\n",
        "# The side-effects appear during the first run\n",
        "print (\"First call: \", jit(impure_print_side_effect)(4.))\n",
        "\n",
        "# Subsequent runs with parameters of same type and shape may not show the side-effect\n",
        "# This is because JAX now invokes a cached compiled version of the function\n",
        "print (\"Second call: \", jit(impure_print_side_effect)(5.))\n",
        "\n",
        "# JAX re-runs the Python function when the type or shape of the argument changes\n",
        "print (\"Third call, different type: \", jit(impure_print_side_effect)(jnp.array([5.])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh7AFNgupwEs",
        "outputId": "38662485-eb69-49e3-85eb-19cf0b261b82"
      },
      "outputs": [],
      "source": [
        "g = 0.\n",
        "\n",
        "def impure_uses_globals(x):\n",
        "    return x + g  # Violating both #1 and #2\n",
        "\n",
        "# JAX captures the value of the global during the first run\n",
        "print (\"First call: \", jit(impure_uses_globals)(4.))\n",
        "\n",
        "# Let's update the global!\n",
        "g = 10.\n",
        "\n",
        "# Subsequent runs may silently use the cached value of the globals\n",
        "print (\"Second call: \", jit(impure_uses_globals)(4.))\n",
        "\n",
        "# JAX re-runs the Python function when the type or shape of the argument changes\n",
        "# This will end up reading the latest value of the global\n",
        "print (\"Third call, different type: \", jit(impure_uses_globals)(jnp.array([4.])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk9Sf4ZUrI8p"
      },
      "source": [
        "in JAX, we need code to be:\n",
        "1. reproducible\n",
        "2. parallelizable\n",
        "3. vectorisable\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using vmap for automatic vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider the following simple code that computes the convolution of two one-dimensional vectors:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "x = jnp.arange(5)\n",
        "w = jnp.array([2., 3., 4.])\n",
        "\n",
        "def convolve(x, w):\n",
        "  output = []\n",
        "  for i in range(1, len(x)-1):\n",
        "    output.append(jnp.dot(x[i-1:i+2], w))\n",
        "  return jnp.array(output)\n",
        "\n",
        "convolve(x, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Suppose we would like to apply this function to a batch of weights w to a batch of vectors x.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xs = jnp.stack([x, x])\n",
        "ws = jnp.stack([w, w])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The most naive option would be to simply loop over the batch in Python. This produces the correct result, however it is not very efficient. In JAX, the jax.vmap transformation is designed to generate such a vectorized implementation of a function automatically:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "auto_batch_convolve = jax.vmap(convolve)\n",
        "\n",
        "auto_batch_convolve(xs, ws)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utsa_3K_GOxp"
      },
      "source": [
        "## Training our first JAX ML model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sadwu9RcoH_n"
      },
      "source": [
        "Let's start with linear regression, $y=wx+b$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "ESpfiL8iGLbj",
        "outputId": "bf49ed2c-8b1c-4dc4-dfdd-5698371152ff"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "xs = np.random.normal(size=(100,))\n",
        "noise = np.random.normal(scale=0.1, size=(100,))\n",
        "ys = xs * 3 - 1 + noise\n",
        "\n",
        "plt.scatter(xs, ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5w_K1aA6uaRd"
      },
      "outputs": [],
      "source": [
        "def model(theta, x):\n",
        "  \"\"\"Computes wx + b on a batch of input x.\"\"\"\n",
        "  w, b = theta\n",
        "  return w * x + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wSgxrZhuk1e"
      },
      "source": [
        "The loss function is the squared error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bBO2EyPuied"
      },
      "outputs": [],
      "source": [
        "def loss_fn(theta, x, y):\n",
        "  prediction = model(theta, x)\n",
        "  return jnp.mean((prediction-y)**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15GQFdkduyLG"
      },
      "source": [
        "Make a step in the direction of the steepest descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQBm-uF9uxJp"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def update(theta, x, y, lr=0.1):\n",
        "  return theta - lr * jax.grad(loss_fn)(theta, x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "KYn3pw4Uu5n-",
        "outputId": "2c4586eb-4c44-4244-8ae6-cb621947d87b"
      },
      "outputs": [],
      "source": [
        "theta = jnp.array([1., 1.])\n",
        "\n",
        "for _ in range(1000):\n",
        "  theta = update(theta, xs, ys)\n",
        "\n",
        "plt.scatter(xs, ys)\n",
        "plt.plot(xs, model(theta, xs))\n",
        "\n",
        "w, b = theta\n",
        "print(f\"w: {w:<.4f}, b: {b:<.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91tn_25vdrNf"
      },
      "source": [
        "Note that the old array was untouched, so there is no side-effect:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GPcIkBI8wNv"
      },
      "source": [
        "## Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxHcqF5t9PTH"
      },
      "source": [
        "Consider the following convolution function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grXT9zZb9Opm",
        "outputId": "5984fbb6-4d2d-4f2b-b092-49a997b2945f"
      },
      "outputs": [],
      "source": [
        "x = jnp.arange(5)\n",
        "w = jnp.array([2., 3., 4.])\n",
        "\n",
        "def convolve(x, w):\n",
        "  output = []\n",
        "  for i in range(1, len(x)-1):\n",
        "    output.append(jnp.dot(x[i-1:i+2], w))\n",
        "  return jnp.array(output)\n",
        "\n",
        "convolve(x, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCuqg8Fj9cQQ"
      },
      "source": [
        "Suppose we need to convolve a batch of weights w to a batch of vectors x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKiRPo5x9bq4"
      },
      "outputs": [],
      "source": [
        "xs = jnp.stack([x, x])\n",
        "ws = jnp.stack([w, w])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRCm4zt19vd-",
        "outputId": "e34b906c-6944-49da-a87c-bc1e5530c497"
      },
      "outputs": [],
      "source": [
        "print(\"xs=\",xs)\n",
        "print(\"ws=\",ws)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU5sYzP89k1I"
      },
      "source": [
        "The most naive option to compute the convolution on each batch would be to loop over batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiOpCd8k98YW"
      },
      "outputs": [],
      "source": [
        "def manually_batched_convolve(xs, ws):\n",
        "  output = []\n",
        "  for i in range(xs.shape[0]):\n",
        "    output.append(convolve(xs[i], ws[i]))\n",
        "  return jnp.stack(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbvcv-eA-ATM",
        "outputId": "e4d8a8a3-5ee0-42e2-979e-76a1c6d1583a"
      },
      "outputs": [],
      "source": [
        "manually_batched_convolve(xs, ws)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgCEJ0O7-FvA"
      },
      "source": [
        "In order to batch the computation efficiently, you would normally have to rewrite the function manually to ensure it is done in vectorised form.\n",
        "\n",
        "This is of course doable, but will require having to change how we deal with indices, axes, etc ...\n",
        "\n",
        "For instance we can vectorise the computation across the batch dimension as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4eGvUTzCPp0",
        "outputId": "97f75960-2e1d-48f0-de82-199e70ba220a"
      },
      "outputs": [],
      "source": [
        "def manually_vectorised_convolve(xs, ws):\n",
        "  output = []\n",
        "  for i in range(1, xs.shape[-1] -1):\n",
        "    output.append(jnp.sum(xs[:, i-1:i+2] * ws, axis=1))\n",
        "  return jnp.stack(output, axis=1)\n",
        "\n",
        "manually_vectorised_convolve(xs, ws)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqpPiOcFCVfO"
      },
      "source": [
        "Automatic vectorization with `vmap`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2oBKivbCcYz",
        "outputId": "ff14ef4d-9b84-4189-fdc0-614a91059db5"
      },
      "outputs": [],
      "source": [
        "auto_batch_convolve = jax.vmap(convolve)\n",
        "\n",
        "auto_batch_convolve(xs, ws)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxaT1P9SCyja"
      },
      "source": [
        "What if we always want to apply convolve a single set of weights and an entire batch of vectors x?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zoackn_XC8rv",
        "outputId": "86d2961a-0d5f-421c-aca7-e69dd307dcc1"
      },
      "outputs": [],
      "source": [
        "batch_convolve_v3 = jax.vmap(convolve, in_axes=[0, None])\n",
        "\n",
        "batch_convolve_v3(xs, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjr2BUcODB66"
      },
      "source": [
        "You can jit vmapped functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc-eQmKKDLzn",
        "outputId": "867cf979-7b5a-4f8f-a072-3f5d31e28397"
      },
      "outputs": [],
      "source": [
        "jitted_batch_convolve = jax.jit(auto_batch_convolve)\n",
        "\n",
        "jitted_batch_convolve(xs,ws)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
